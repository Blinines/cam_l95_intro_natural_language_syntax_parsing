General 
- http://kodu.ut.ee/~treumuth/NLP/syntax.pdf

For conversion script
' and " 
e.g. raising errors

For the report




Introduction

Background


https://nlp.stanford.edu/software/dependencies_manual.pdf


Gold standard => Stanford dependencies, original version.
https://nlp.stanford.edu/software/dependencies_manual.pdf
From version 3.5.2, default representation is the new UD (representation used in Benepar and StanfordNLP)
https://nlp.stanford.edu/pubs/USD_LREC14_paper_camera_ready.pdf

Major changes
- treatment of prepositions and case marking. No prep/pobj, instead any case-marking element (prepositions,
postpositions, clitic case markers) = dependent of the noun it attaches to or introduces 
*nmod* labels relations between two content words, *prep* case depending on its complement


When comparing gold standard to references
- correct if same head and dependency, and same dependency relation (either the same, either equivalent regarding
the changes between original SD and UD)
- summed up in a table


Notes on Benepar sentences
- sent-1. Different treatment of prepositions leads to different head tagging. All OK.
- sent-2. Whether 'might be snow' should have had a 'cop' tag for the snow, parser and gold standard yield different results
which shed light to the ambiguity. About the same. stays (about the same) (over ...) or stays (about the same over ())
- sent-3. 10K versus 10 K but same analysis, counting it as one for the accuracies. to aux vs mark, POS nn vs dep because
unknown, per second to words or and, only to 1 or MB, uses only 1 MB of RAM, parser faultly assigning head to adverbs 
or punct.
- sent-4. Parser finds two sentences hence two root structures. punct to root for the parser ? including interjections
vmod vs case + nmod. Struggles to deal with parenthesis.
- sent-5. Two sentences idied likewise for the gold standard and the parser. prt syntax, difficulties with `the original
Brown and C5 tagsets`, punctuations head, C5 as nummod for example, parser heading many tokens to the 'each'
- sent-6. Confusions with the mathematical formulas like O=(o1,o2,..oT) hence creates new sentences when there is the =
sign. For more adequate comparison with the gold standard, we remove the = part to get the new parsing. i.e. keeping only
O and Q. Most probablec tag/sequence => does not analyse correctly the head, mainly because of this /. head of algorithm,
most probable state, together with.
- sent-7. Parser uses three token for composed verbs used as adjectives (EM-trained, hand-tagged), hence in order to
compare to the gold standard, we just use trained and tagges in order to have the same number of words. Punctuation.
Is probably best suited : advmod vs amod, passive vs active form. Adverb vs adjective.
- sent-8. Parser detects two sentences because of the dot hence last sentence is alone. In the gold standard we chose
to use parataxis instead. proceed (to fashlane) (under) vs (proceed (to fashlane (under)))
- sent-9. Operates ccomp vs dep. Punctuation same problem. advcl vs ccomp.
- sent-10. MWE, pcomp vs advcl (constantly worrying + cf one example in previous example), mark


Notes on StanfordNLP sentences
- sent-1. 
- sent-2. neg vs advmod, advmod+advcomp + prepositional group, stays (the same)(over) vs stays(the same over)
- sent-3. nummod/compound for 10 K, gold 10K, parser 10 K. 10 - 13 - compound and K - 14 - nummod hence compressing
into 10K - 14 - nummod.is to build aux vs mark. Wrong heads for punct + cc + advmod. acomp vs advmod.
- sent-4. Parser, unlike Benepar, finds only one sentence. (more or less unique)function vs (more or less)(unique)function,
including interjections, discourse/parataxis tagging, ... (there are horses) among others.
- sent-5. 87-tag tokenised into three sequences 87, - and tag. 87 and -'s head is tag hence we just put on this value. prt
vs case for 'was culled from' (same than for Benepar), for long sequences with a lot of nn often don't find the right head
of NP + consequently right dep + difficulties of parenthesis
- sent-6. Same remark than for the Benepar parser, however interesting to see that are parsed differently. => applying the
same new parsing than for Benepar. That we present, long NP often identifies the head as the last part but not always
the case, hence bad labelling for conj/cc, state/tag, together with
- sent-7. Same than for the Benepar parser. Reparsing to get better comparison with gold standard. Is best-suited should
be active or passive?, negation as determiners, amod/advmod/mark
- sent-8. Same than Benepar regarding the two sentences. conj and punct.
- sent-9. cc/punct (provide NB, not most important dependency relation hence ok), simply defining what the media lab is
- sent-10. Instead of constantly worrying, have to convince companies

Notes on RASP sentences
- General : RASP notations do not use punctuation tagging, hence disregarded for this part of study.
- sent-1. Head for gold standard = car, =was for RASP system, just different notations. ncsubj = nsubj, prep and iobj,
dobj and pobj, at auction (last year)(in February) vs at auction (last year in February)
- sent-2. be (snow)(on high ground) vs be (snow on high ground). about the same differences between RASP and SD in terms
of notations
- sent-3. Poss treated as det for rasp notations. `it may prove too hard` relating to the root (gold) or to the using part 
only (rasp). Conj and nsubj problem for rasp. 
- sent-4. Deals with () differently => not taken into account. For RASP out of 53 tokens, 24 not tagged. Does not deal
well with long dependencies, conj/cc, lots of variants for RASP's ncmod in SD.
- sent-5. RASP interprets : (Brown)&(C5 tagsets). 2nd sentence wrong head. include of. (C5)(tag)(VDD), (C5 tag)(VDD).
Some tokens not tagged in RASP version (excluding punctuation, 8/53). 
sent-6. Same remark than for the last two parsers. => modifying the sentence so that it can work. interpreted by the RASP
parser : (we present ((takes as input an HMM)and(a sequence of words))and(returns the most probable state). Structure as 
a whole not coherent. + RASP does not analyze all the words, making it harder to find the root for example. Unclear about
the root.
- sent-7. Contrary to the two other parsers, keep words like hand-tagged together. in RASP : Thus linked to EM-trained.
long nn sequence hard to decompose correctly for the parser. 
- sent-8. proceed (to faslane under) vs proceed(to faslane)(under)
- sent-9. RASP does not tokenise everything, seems to `forget` beginning-of-sentence conj like 'but'. => not taken into
account for comparison. The Media Lab do not recognize the head of NP. 
- sent-10. start or continue their support. 